---
layout: draft
title: "GAN"
date: 2020-03-22 14:49:0 +0000
comments: False
share: False
categories: cv
---

[UCB20, CS294-158 Deep Unsupervised Learning Lecture 5 & 6](https://drive.google.com/file/d/1qCVpu2zFz1uEe3QcNHGlaT1Rs2u8HrCc/view)




[Mimicry: A PyTorch GAN library that reproduces research results for popular GANs.](https://github.com/kwotsin/mimicry)


**[Contrastive Generative Adversarial Networks,Arxiv2006](https://arxiv.org/pdf/2006.12681.pdf)**


**[MixNMatch: Multifactor Disentanglement and Encoding for Conditional Image Generation,CVPR20](https://arxiv.org/pdf/1911.11758.pdf)**

better on flow model?


**[SEAN: Image Synthesis with Semantic Region-Adaptive Normalization,CVPR20,oral](https://arxiv.org/pdf/1911.12861.pdf)**

**[SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient,AAAI17](https://arxiv.org/abs/1609.05473)**

**[Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer,ICLR19](https://openreview.net/forum?id=S1fQSiCcYm)**

> In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. 

![](/imgs/mixup-autoencoder.png)

- the task of autoencoding 32 Ã— 32 grayscale images of lines is interesting experiments.
- Mixup in AutoEncoder, not VAE. Focused on the bility of interpolation rather than reconstruction/generation.
  > [openview:](https://openreview.net/forum?id=S1fQSiCcYm&noteId=SJgQLPtKTm)In general we do not expect this regularizer to improve the sample quality of a given autoencoder, since the critic's primary objective is to discriminate between interpolants and reconstructions (not interpolants and "real" data). The goal instead is to take an autoencoder which already reconstructs well but interpolates poorly and improve the quality of the interpolations. The VAE typically has the opposite problem - it reconstructs poorly but interpolates smoothly. In other words, the latent space of the VAE is already "continuous" in some sense (due to the enforcement of the prior) but many regions in latent space map to "unrealistic" (i.e. blurry) outputs. So, we aren't sure whether our regularizer would improve VAE reconstructions. 

- similar work: [On Adversarial Mixup Resynthesis,NeurIPS19](https://papers.nips.cc/paper/8686-on-adversarial-mixup-resynthesis)
> methodologically, the main difference and advantage of the proposed method over ACAI lie in the flexibility of mixing functions, but in the full training data setting (Table 1) it seems the simplest mixup performs the best. This suggests that it may not be necessary to use different mixing functions. While the proposed method still outperforms ACAI in this case, the gain is more from a different GAN variant under the ACAI framework than a different mixing function, which is less novel and interesting.


**[CNN-generated images are surprisingly easy to spot... for now,CVPR20](https://arxiv.org/pdf/1912.11035.pdf)**

classifiers trained to detect CNN-generated images can exhibit a surprising amount of generalization ability across datasets, architectures, and tasks.

**[Seeing What a GAN Cannot Generate,ICCV19](http://ganseeing.csail.mit.edu/)**

- What does a GAN miss in its overall distribution?
- What does a GAN miss in each individual image?


